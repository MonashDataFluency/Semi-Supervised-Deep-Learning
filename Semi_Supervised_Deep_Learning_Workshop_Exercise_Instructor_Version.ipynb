{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semi Supervised Deep Learning Workshop Exercise - Instructor Version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm8zOxOwCIsk"
      },
      "source": [
        "## Semi Supervised Deep Learning Workshop Exercise - Instructor Version\r\n",
        "\r\n",
        "This notebook contains example code to complete the basic exercises in the hands-on component of the Semi-Supervised Deep Learning workshop.\r\n",
        "For complete context, refer to the student version of this notebook. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMqrh55TCHlA"
      },
      "source": [
        "# Load the modules we will be using today\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow.keras.layers as L\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tensorflow import keras\r\n",
        "import random\r\n",
        "import pickle\r\n",
        "\r\n",
        "# Preprocess the dataset for convenience\r\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\r\n",
        "\r\n",
        "# Normalise the images and remove redundant dimensions\r\n",
        "train_images = train_images / 255.0\r\n",
        "test_images = test_images / 255.0\r\n",
        "train_labels = np.squeeze(train_labels)\r\n",
        "test_labels = np.squeeze(test_labels)\r\n",
        "\r\n",
        "# Populate the \"dataset\" dictionary with the CIFAR-10 data\r\n",
        "dataset = {}\r\n",
        "dataset['train_images'] = train_images\r\n",
        "dataset['train_labels'] = train_labels\r\n",
        "dataset['train_images_grouped'] = [ [] for i in range(10) ]\r\n",
        "dataset['test_images'] = test_images\r\n",
        "dataset['test_labels'] = test_labels\r\n",
        "\r\n",
        "for i in range(train_labels.shape[0]):\r\n",
        "  dataset['train_images_grouped'][train_labels[i]].append(train_images[i])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqIFi6bSJY5a"
      },
      "source": [
        "# Inspect the dataset\r\n",
        "print('Dataset keys:', [k for k in dataset.keys()])\r\n",
        "print('Training images per class:', [len(c) for c in dataset['train_images_grouped']])\r\n",
        "print('Image shape:', dataset['train_images_grouped'][0][0].shape)\r\n",
        "\r\n",
        "fig=plt.figure(figsize=(20, 20))\r\n",
        "rows = 10 # Show all 10 classes\r\n",
        "columns = 5 # Show 5 samples per class\r\n",
        "for i in range(rows):\r\n",
        "  for j in range(columns):\r\n",
        "    fig.add_subplot(rows, columns, (i)*columns+j+1)\r\n",
        "    plt.imshow(dataset['train_images_grouped'][i][j])\r\n",
        "plt.show()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4-N7oeLUh-N"
      },
      "source": [
        "# Extract the first 1% of labelled samples\r\n",
        "take_first_X = 50\r\n",
        "train_images_supervised = []; train_labels_supervised = []\r\n",
        "for i in range(10):\r\n",
        "    train_images_supervised += dataset['train_images_grouped'][i][:take_first_X]\r\n",
        "    train_labels_supervised += [i] * take_first_X\r\n",
        "\r\n",
        "train_images_supervised = np.asarray(train_images_supervised)\r\n",
        "train_labels_supervised = np.asarray(train_labels_supervised, np.int32)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajY1TGteSL0Z"
      },
      "source": [
        "# Define the model architecture\r\n",
        "class SimpleModel(tf.keras.Model):\r\n",
        "  def __init__(self):\r\n",
        "    super(SimpleModel, self).__init__()\r\n",
        "\r\n",
        "    # Encoder layers\r\n",
        "    # Hint: You may choose to use keras.layers.Conv2D\r\n",
        "    self.enConv1 = L.Conv2D(16,(3,3), padding='same', activation='relu')\r\n",
        "    self.enConv2 = L.Conv2D(32,(3,3), padding='same', activation='relu')\r\n",
        "    self.enConv3 = L.Conv2D(64,(3,3), padding='same', activation='relu')\r\n",
        "\r\n",
        "    # Classification head layers\r\n",
        "    # Hint: You may choose to use keras.layers.Dense\r\n",
        "    self.dense1 = L.Dense(32, activation='relu')\r\n",
        "    self.dense2 = L.Dense(10, activation=None)\r\n",
        "\r\n",
        "    # Decoder head layers\r\n",
        "    # Hint: You may choose to use keras.layers.Conv2DTranspose\r\n",
        "    # Creating an output the same shape as the input using deconvolution layers may be tricky\r\n",
        "    # Using padding='same' makes it easier.\r\n",
        "    self.deConv1 = L.Conv2DTranspose(32, (3,3), padding='same', strides=(2,2), activation='relu')\r\n",
        "    self.deConv2 = L.Conv2DTranspose(1, (3,3), padding='same', strides=(2,2), activation='sigmoid')\r\n",
        "\r\n",
        "  def call(self, x):\r\n",
        "    # Encoder\r\n",
        "    x = self.enConv1(x)\r\n",
        "    x = L.MaxPooling2D((2, 2), strides=(2, 2))(x)\r\n",
        "    x = self.enConv2(x)\r\n",
        "    x = L.MaxPooling2D((2, 2), strides=(2, 2))(x)\r\n",
        "    x = self.enConv3(x)\r\n",
        "\r\n",
        "    # Classification head\r\n",
        "    c = L.Flatten()(x)\r\n",
        "    c = self.dense1(c)\r\n",
        "    c = self.dense2(c)\r\n",
        "\r\n",
        "    # Decoder head\r\n",
        "    s = self.deConv1(x)\r\n",
        "    s = self.deConv2(s)\r\n",
        "\r\n",
        "    return c, s\r\n",
        "\r\n",
        "# Define the loss function\r\n",
        "def SemiSupervised_CrossEntropy_or_MSE(labels, inputs, logits, regen_img, mode='supervised'):\r\n",
        "\r\n",
        "    if mode == 'supervised':\r\n",
        "      # Classification loss is necessary since this is the supervised loss\r\n",
        "      classification_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits))\r\n",
        "\r\n",
        "      # Regeneration loss is not strictly necessary\r\n",
        "      reconstruction_loss = tf.reduce_mean(tf.square(inputs-regen_img))\r\n",
        "\r\n",
        "      return classification_loss + reconstruction_loss\r\n",
        "\r\n",
        "    else:\r\n",
        "      # Classification loss is 0 as it is not used here\r\n",
        "      classification_loss = tf.reduce_mean(0.*logits)\r\n",
        "\r\n",
        "      # Regeneration loss is simply the L2 loss\r\n",
        "      reconstruction_loss = tf.reduce_mean(tf.square(inputs-regen_img))\r\n",
        "\r\n",
        "      return classification_loss + reconstruction_loss\r\n",
        "\r\n",
        "# Define a validation function\r\n",
        "def ValidateModel(model, test_images, test_labels):\r\n",
        "  predictions, seg = model.predict(test_images)\r\n",
        "  predicted_classes = np.argmax(predictions, axis=-1)\r\n",
        "  accuracy = 1. * np.sum(predicted_classes == test_labels) / test_labels.shape[0]\r\n",
        "  print('Validation accuracy =', accuracy)\r\n",
        "  confusion_matrix = np.zeros((10,10))\r\n",
        "  for i in range(test_labels.shape[0]):\r\n",
        "    confusion_matrix[test_labels[i],predicted_classes[i]] += 1\r\n",
        "  print(confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UUd03rL3WRe"
      },
      "source": [
        "# The main training script\n",
        "\n",
        "model = SimpleModel()\n",
        "optimiser = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "accumulated_loss = 0\n",
        "supervised_prob = 50  # Probability of sampling a supervised batch for training\n",
        "max_train_batches = 2000\n",
        "batch_size = 32\n",
        "\n",
        "train_batch = 1\n",
        "while train_batch < max_train_batches:\n",
        "\n",
        "    # Example of increasing the supervised probability throughout training\n",
        "    if train_batch % 250 == 0:\n",
        "      supervised_prob += 10\n",
        "\n",
        "    x = np.random.randint(0,100)\n",
        "    train_batch += 1\n",
        "    if x < supervised_prob: # if supervised\n",
        "      \n",
        "      input_batch = []; label_batch = []\n",
        "      for i in range(batch_size):\n",
        "        x = np.random.randint(0, train_images_supervised.shape[0])\n",
        "        input_batch.append(train_images_supervised[x])\n",
        "        label_batch.append(train_labels_supervised[x])\n",
        "        \n",
        "    else:\n",
        "        input_batch = []\n",
        "        for i in range(batch_size):\n",
        "          x = np.random.randint(0, dataset['train_images'].shape[0])\n",
        "          input_batch.append(dataset['train_images'][x])\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      input_batch = np.asarray(input_batch)\n",
        "      logits, regen_img = model(input_batch, training=True)\n",
        "      \n",
        "      if x < supervised_prob:\n",
        "          loss = SemiSupervised_CrossEntropy_or_MSE(np.asarray(label_batch), input_batch, logits, regen_img,\n",
        "                                                                  mode='supervised')\n",
        "      else: # Unsupervised mode\n",
        "          loss = SemiSupervised_CrossEntropy_or_MSE(None, input_batch, logits, regen_img,\n",
        "                                                                  mode='unsupervised')\n",
        "          \n",
        "      gradients = tape.gradient(loss, model.trainable_variables)\n",
        "      optimiser.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    accumulated_loss += loss.numpy()\n",
        "\n",
        "    # Validation at every certain number of batches\n",
        "    if train_batch % 250 == 0:\n",
        "      print('Validation at train batch', train_batch)\n",
        "      ValidateModel(model, dataset['test_images'], dataset['test_labels'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VURMMCV48N-I"
      },
      "source": [
        "### Miscellaneous example code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkN7shTctK3M"
      },
      "source": [
        "# A function to randomly shuffle two associated lists (such as images and labels) in the same order\n",
        "def randshuf(a,b):\n",
        "  c = list(zip(a,b))\n",
        "  random.shuffle(c)\n",
        "  a,b = zip(*c)\n",
        "  return a,b\n",
        "\n",
        "# A function to add Gaussian noise to the input images\n",
        "def AddNoise(images, range):\n",
        "  for i, img in enumerate(images):\n",
        "    img += np.random.normal(loc=0.0, scale=range, size=img.shape)\n",
        "  return images\n",
        "\n",
        "# Or add noise inline as follows:\n",
        "# Be sure to not modify the original dataset as it will corrupt the data for future use\n",
        "input_batch_noise = input_batch + np.random.normal(scale=0.0, size=input_batch.shape)\n",
        "test_images_noise = test_images + np.random.normal(scale=0.0, size=test_images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}